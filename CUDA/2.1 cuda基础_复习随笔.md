调度单位：warp

执行单位：thread

warp divergence(分支)

分配grid,block<100,1>，会发生什么



线程块：warp，sharedmemory,线程块内线程同步，线程块的线程在同一个Stream Multiprocessor同步



grid：执行相同kernel，线程块可以在多个SM并行执行



threadIdx,blockIdx,blockDim,gridDim



cudaruntime library:

* cudamalloc
* cudamemcpy
* cudafree
* cuda errorcheck
* cuda kernel clock
* cudaMemset
* cudaEventCreate(),cudaEventRecord(),cudaEventSynchronize(),cudaEventElapsedTime(ptr,Eventstart,Eventend),cudaEventDestroy

```c++
//这行代码声明了两个CUDA事件变量：start 和 stop。
cudaEvent_t start, stop;
//创建一个新的事件对象并将其与start关联。此事件将用于记录内核执行的开始时间。
cudaEventCreate(&start);
//创建一个新的事件对象并将其与stop关联。此事件将用于记录内核执行的结束时间。
cudaEventCreate(&stop);
//记录当前时间到start事件。这意味着从此刻开始，我们开始测量时间。
cudaEventRecord(start);
//这行代码启动CUDA内核CUDAKernel。Grid和Block定义了内核的执行配置，即网格和块的大小。
CUDAKernel<<<Grid, Block>>>();
// 在内核执行完成后，记录当前时间到stop事件。这意味着我们停止测量时间。
cudaEventRecord(stop);
//这是一个同步调用，它确保stop事件已经被记录。因为CUDA操作是异步的，所以在继续执行任何其他操作之前，我们需要确保stop事件已经完成。这确保了我们得到的时间测量是准确的。
cudaEventSynchronize(stop);//让CPU等待stop被记录
//这行代码计算start和stop之间经过的时间（以毫秒为单位）并将结果存储在milliseconds变量中。这给了我们内核执行的准确时间
cudaEventElapsedTime(&milliseconds, start, stop);
```

> 在CUDA中，当你调用一个内核或任何其他设备操作时，这些操作不是立即执行的。相反，它们被排入一个队列中，称为流（stream）。默认情况下，所有操作都在默认流中执行，除非你明确地创建和使用其他流。
>
> 当你调用`cudaEventRecord(stop)`，`stop`事件被插入到当前流中，紧随其后的是之前调用的`CUDAKernel`内核。这意味着，一旦`CUDAKernel`内核完成其执行，`stop`事件就会被记录。
>
> 这是一个简化的描述：
>
> 1. `CUDAKernel<<<Grid, Block>>>();` - 这行代码将`CUDAKernel`内核插入到流中。
> 2. `cudaEventRecord(stop);` - 这行代码将`stop`事件插入到相同的流中，紧随其后。
>
> 由于CUDA操作在流中是按顺序执行的，所以`stop`事件只有在`CUDAKernel`内核完成执行后才会被记录。这确保了`stop`事件记录的时间点正好是内核执行完成的时间点。
>
> 当你稍后调用`cudaEventSynchronize(stop)`时，它会阻塞CPU的执行，直到`stop`事件在流中被记录，这进一步确保了你得到的时间测量是准确的。

* CUDA main函数流程

* dim3类型

* cudaError_t <cudafunction>;

* 内存事务Memory Coalescing/缓存行cacheline

  * 自动进行

  * 对各种内存的Read Write

  * 当一个warp（在CUDA中，一个warp是一组同时执行的线程，数量通常为32）中的线程访问全局内存时，如果这些访问是对连续内存地址的，则GPU可以将这些访问合并为一个更大的内存访问事务。这意味着，GPU不需要为每个线程单独发起一个内存事务，而是一次性地加载一个数据块，这个数据块包含了所有线程请求的数据。==不同wrap的thread不能够内存合并==

  * GPU内存访问是以事务为单位进行的。一个内存事务可以传输一定范围内的数据，例如，128位、64位或32位等。当多个线程访问相邻的内存地址时，这些访问可以被合并为较少的内存事务，从而提高访问效率。

  * 内存事务合并主要是通过合并内存访问请求来优化全局内存访问的效率，减少内存访问次数和提高带宽利用率。而缓存命中则是利用近距离数据的时间和空间局部性原理，通过保存常用数据到快速访问的缓存中来减少数据访问延迟。

  * 内存事务和Cache Hit不同之处在于，内存事务直接减少了访问的次数，而CacheHit减少了从内存寻找数据的次数。

  * 虽然内存事务合并是自动进行的，使用`float4`（128位）或类似的向量类型（如`int4`、`double2`等）进行手动优化仍然是有益的。原因在于，通过使用这些向量类型，开发者可以更有效地利用内存带宽，减少内存访问次数，并显式地指导合并事务的发生。并且GPU有些支持单指令处理float4

    > ### 数据加载到寄存器
    >
    > 当一个warp执行一次合并的内存事务，从全局内存中取得一块连续的数据后，这块数据首先被加载到GPU内部的寄存器或共享内存中。每个线程都有自己的寄存器集，而共享内存则是由同一个block内的所有线程共享
    >
    > ### 数据的分配
    >
    > 1. **基于索引的分配**：每个线程在warp中都有一个唯一的线程索引（Thread ID），这个索引决定了它将从加载的数据块中获取哪一部分数据。例如，如果一个warp的线程同时请求一个数组的连续部分，那么线程ID为0的线程将获取第一个数据元素，线程ID为1的线程将获取第二个数据元素，以此类推。这种分配机制是自动进行的，开发者不需要编写额外的代码来实现这一点。
    > 2. **寄存器中的数据**：每个线程根据自己的线程索引获得相应的数据后，这些数据通常存储在各自的寄存器中。寄存器是每个线程私有的高速存储区，可以提供非常快速的数据访问速度。
    > 3. **共享内存的使用**：在某些情况下，合并加载的数据可能首先被放置在共享内存中，然后由每个线程根据需要读取。共享内存的使用可以减少全局内存访问次数，并且比从全局内存读取数据要快很多，但它需要开发者在CUDA核函数中显式地编程实现。











* ```c++
  // 需要判断一定要是在范围内，在kernel中对float4做类似for展开的操作
  float* targetarray;//targetarray.x,.y,.z,.w
  std::cout << reinterpret_cast<float4*>(targetarray)[i]<<std::endl;//这种方法直接使用危险的强制类型转换，读取第i个float4
  
  
  //
  __global__ void mem_bw(float* A, float* B, float* C, int size) {
  	int idx = blockIdx.x * blockDim.x + threadIdx.x;
  
  	if (idx < size) {
  		float4 a1 = reinterpret_cast<float4*>(A)[idx];
  		float4 b1 = reinterpret_cast<float4*>(B)[idx];
  		float4 c1;
  		c1.x = a1.x + b1.x;
  		c1.y = a1.y + b1.y;
  		c1.z = a1.z + b1.z;
  		c1.w = a1.w + b1.w;
  
  		reinterpret_cast<float4*>(C)[idx];
  	}
  }
  ```







* 有空了解：内联PTX（Parallel Thread Execution）汇编

  ```c++
  __device__ __forceinline__
  float4 LoadFromGlobalPTX(float4 *ptr) {
      float4 ret;
      asm volatile (
          "ld.global.v4.f32 {%0, %1, %2, %3}, [%4];"
          : "=f"(ret.x), "=f"(ret.y), "=f"(ret.z), "=f"(ret.w)
          : "l"(ptr)
      );
  
      return ret;
  }
  ```

  > ### 使用CUDA事件测量执行时间
  >
  > - 创建两个CUDA事件`start`和`stop`，用于标记性能测试的开始和结束时间点。
  > - 通过`cudaEventRecord`在执行内核前后分别记录这两个事件，并使用`cudaEventSynchronize`等待事件完成，以确保准确测量内核执行时间。
  > - 使用`cudaEventElapsedTime`计算`start`和`stop`之间的时间差，得到内核执行的总时间（毫秒）。
  >
  > ### 计算显存带宽
  >
  > - 根据执行的操作（加法或复制）和处理的数据量计算显存带宽。在这个例子中，每次内核执行处理的数据量是`ARRAY_SIZE`大小的三倍（因为每个操作涉及两个输入数组和一个输出数组），并且这个操作被重复了`BENCH_ITER`次。
  > - 显存带宽（GB/sec）的计算公式是`3 * ARRAY_SIZE * sizeof(float) * BENCH_ITER / (milliseconds * 1e6)`，其中`3`表示每次操作涉及的数组数目，`sizeof(float)`是每个数组元素的大小，`milliseconds`是内核执行时间，`1e6`用于将带宽单位转换为GB/sec。





* 带宽：

  * 带宽是指在单位时间内可以传输的数据量，通常以GB/s（吉字节每秒）或其他相应的单位来表示。
  * 例如，如果一个GPU的内存带宽为288 GB/s，这意味着理论上它每秒可以读取或写入288吉字节的数据。

* ErrorCheck:

  ```c++
  // 声明
  cudaError error;
  // 打印错误
  error = cudaMalloc((int**)&d_a, NO_BYTES);//==例如==此处就可以使用
  	if (error != cudaSuccess) {
  		fprintf(stderr, "Error %s \n", cudaGetErrorString(error));
  	}
  ```





* **关于线程和线程块的资源限制**:
  - 每个核函数的线程和线程块数量都有其限制，这些限制是针对单个核函数的。
  - 如果一个核函数使用了所有的资源，那么下一个核函数必须等待这些资源被释放才能开始执行。
  
* **关于CUDA资源的复用**:
  - 线程和线程块的资源（如寄存器和共享内存）在核函数执行完毕后会被释放，并为后续的核函数重新分配。
  - 设备内存（Device Memory）在核函数执行完毕后不会自动释放，除非显式地进行释放操作。这意味着设备内存可以在不同的核函数之间被复用，但需要确保不会发生数据竞争或不一致。
  
* 获取GPU信息

* 启动kernel->分配block给SM->warp scheduler从SM的warp pool选择warp处理->dispatching unit分配warp的thread给执行单位(warp中的所有线程在同一时间执行相同的指令，但在不同的数据上。)

  * 不同的SM之间warp不能自由切换

    

* 计算峰值浮点性能时的公式可能如下：

  $$PeakFLOPS=core频率*SM个数*吞吐*2 = F_{clk}*N_{SM}*T_{ins}*2$$​

  这里公式中的乘以2通常是因为每个浮点操作可以在每个时钟周期内执行两个操作：一次加法（Add）和一次乘法（Multiply）。这是在现代GPU架构中常见的，尤其是在执行复合浮点操作，

  ==吞吐：这意味着每个SM每个时钟周期可以执行多少次浮点操作，这里的“n次操作”并不是指单个核心在一个周期内执行64次操作，而是整个SM在同一时钟周期内并行执行的操作总数。==



* SIMD与SIMT的区别
  * multi data, multi thread
  * 前者所有数据元素需要遵循相同的指令流，遇到分支就需要对不需要执行当前分支操作的使用掩码，后者**每个warp**独立处理分支
* warp divergence:为了遵守SIMT执行约定，CUDA运行时将强制每个thread按顺序检查if else中的all the statements ，并且disable那些并没有走这条path的thread。==注意，单纯的条件检查并不会引起wrap divergence，只有有多个执行路径是才会出现。==
  * warp中的所有线程必须同步执行。如果线程需要执行不同的分支，GPU会序列化这些分支的执行。首先执行一条路径上的线程，而其他线程等待；然后执行另一条路径，等等。这减少了并行性，导致效率降低。
* warp上下文存在SM
  * 每个thread处理的越复杂，register消耗的越多，能同时计算的warp越少
  * 每个block越复杂，shared memory消耗的越多，在同一个SM上面的block越少
* warp的几个状态
  * Active Warp：SM收到block，warp分配到了register以及sharedmemory
  * Selected Warp： 被选中执行的warp
  * Stalled Warp: 在等待数据传到这里的warp（包括计算结果或者内存访问）
  * Eligible Warp: 正常的warp，（1）拥有所有的必要资源，（2）并且计算资源有空缺，warp schedular选择的目标
* latency，latency hiding(使用Eligible Warp替代Stalled Warp)
* Occupancy = ActiveWarps/maximumwarps
* GPU memory hirerarcy:global memory, shared memory(bank,bank conflict),register,constant memory
* CUDA stream:cudaStream_t,cudaStreamCreate,cudaMemcpyAsync,cudaStreamSynchronize,cudaStreamDestroy









