## 1. RMSNorm

RMSNorm接受的是Input Embedding的输出

因此其输入的shape就是[number_tokens, q_hidden_units]

因为其是Normalization，因此输出的shape也是q_hidden_units





因为是序列模型，考虑到一个batch内的每个sequence可能长度都不同，因此使用BatchNorm不合理，因此首先考虑使用LayerNormalization。但是，LayerNormalization的计算量十分大，因此选用RMSNormalization来加快计算。


Layer Normalization（层归一化）和 Root Mean Square Layer Normalization（RMSNorm）是两种用于深度学习模型中的归一化技术，尤其是在处理序列数据时，如在自然语言处理任务中使用的变换器（Transformer）模型。这两种技术都是为了减少训练过程中的内部协变量偏移（Internal Covariate Shift），从而加快训练速度，提高模型的稳定性和性能。下面分别介绍这两种技术。

### Layer Normalization (层归一化)

Layer Normalization 通过计算特征维度上的均值和方差来归一化网络层的激活输出。不同于批量归一化（Batch Normalization），层归一化在单个样本的层内进行，因此它独立于批量大小，并且可以很好地用于循环神经网络（RNNs）和Transformer中。

给定一个样本在网络层的输出$x$，层归一化的公式可以表示为：
$$
LN(x) = \gamma \cdot \frac{x-\mu}{\sqrt{\sigma^2 + \epsilon} }+ \beta
$$


其中，

- $\mu$ 是该层内每个样本特征的均值，
- $\sigma^2$是方差，
- $\epsilon$是一个很小的数，防止除以零，
- $\gamma$和 $\beta$是可学习的参数，用于重新缩放和位移，确保归一化操作不会破坏网络的表达能力。

### RMSNorm (根均方层归一化)

RMSNorm 是一种简化的层归一化方法，它仅使用特征维度的根均方（Root Mean Square）进行归一化，而不是使用均值和方差。RMSNorm 主要用于自然语言处理中的Transformer模型，可以减少计算量并提高性能。

RMSNorm 的公式为：
$$
RMSNorm(x) = \gamma \cdot \frac{x_i}{\sqrt{\frac{1}{H}\sum^H_{i=1}x^2_i+\epsilon}}
$$


其中，

- $H$是特征维度的数量，
- $x_i$是特征向量中的第 *i* 个元素，
- $\epsilon$ 是一个很小的数，防止除以零，
- $\gamma$ 是可学习的缩放参数（注意，与层归一化不同，这里没有位移参数 *β*）。

### 比较

#### Layer Normalization的局限性

1. **局限性描述**：LN通过计算层输出的均值（*μ*）和方差（$2σ^2$）来进行去中心化和缩放操作。这种方法可以在一定程度上使模型对输入和权重中的噪声更加鲁棒，即使是在极端分布（如99个0和1个100）下，LN通过中心化保持了分布的稳定性。然而，文中指出，尽管LN可以实现聚集中心化（re-centering），但这并不意味着它能有效减少梯度的方差。因此，LN的成功并不完全依赖于其能够减少梯度方差的能力。
2. **局限性解读**：LN的中心化过程能够调整数据分布，让每一层的输入分布更加稳定，但这种调整对于梯度方差的影响并不显著。这意味着，虽然LN有助于保持模型训练的稳定性，但它并不能直接解决训练过程中可能出现的梯度爆炸或消失问题。
3. 换句话说，LN通过稳定输入的分布，减少了不同层之间输出分布的差异，同时增加了模型对不同分布输入的稳定性，这在序列模型中其实是不需要的。

#### RMSNorm的优势

1. **优势描述**：与LN不同，RMSNorm放弃了中心化操作，只通过均方根（RMS）进行缩放。这意味着，当LN中的均值（*μ*）为0时，RMSNorm与LN实际上是相同的。RMSNorm的关键优势在于它只进行缩放操作，不改变数据的原始分布，这对于激活函数输出的稳定性非常有利。因为缩放操作不会改变向量的方向，它可以保持激活输出的相对分布不变，从而提高模型的学习效率和泛化能力。
2. **优势解读**：RMSNorm的简化过程（仅缩放，不中心化）减少了计算复杂度，同时保持了数据分布的原始特征。这种方法特别适用于深层网络和复杂模型，其中保持激活函数输出的稳定性对于避免梯度问题和加快训练速度至关重要。简言之，RMSNorm通过维持激活输出的稳定性，有助于模型更有效地学习和泛化，而无需担心输入数据分布的大幅度变化。