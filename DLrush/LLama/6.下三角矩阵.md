## 1. mask在哪

![image-20240328160623634](./assets/image-20240328160623634.png)

图中标识出了，Buildmask和使用mask的位置。





注意，mask是在context decoder部分，在selfdecoder部分是没有mask的，因为context decoder需要mask来处理prompt，而selfdecoder是一个自回归，只会给出一个单词，天然的你就看不见后面未生成的token。

![1c34d41bf689f5e0173d6fc012285fb](./assets/1c34d41bf689f5e0173d6fc012285fb.jpg)



在context decoder的推理阶段，用户输入一整个prompt

![6e339df4149ef8a16fa42d570899ced](./assets/6e339df4149ef8a16fa42d570899ced.jpg)

上面这种说法待定，因为有点问题，我认为只有一个decoder。

## 2.代码中mask矩阵的判定

![image-20240401155416385](./assets/image-20240401155416385.png)

这里，上下文的长度为max_k_len - k，根据这个上下文长度，每次多获得一个词，query就多一个，key也多一个。

换句话说，mak_k_len是上下文长度，max_q_len是上下文长度加上回答的长度。

```c++
 bool is_one = q < qlen && k < klen && k<= q + (klen - qlen) && k >= klen-qlen;
```

## 3. Decoder-Only structure & self-masked attention

https://www.youtube.com/watch?v=bQ5BoolX9Ag

简要说一下

Decoder-only:只使用decoder架构

self-masked attention：Decoder-only使用self-masked时候，会将prompt一同预测，如果预测错了也不管它直接跳过，直到预测到答案。

casual mask：也有一种方法，casualmask，它跳过了prompt，直接开始利用已有的prompt和预测的token作为输入，它在训练时用于遮蔽未来的词，但是在推理时，它应该会准确指向最后一个词（假装后面的全被遮蔽了，实际上后面没有，此时相当于普通自回归）





## 4. 中途得到的理解2

首先，LLama是一个Decoder-Only的模型

它使用了KV cache，这意味着处理完Prompt，我们会保留其KV cache，具体看

https://www.bilibili.com/video/BV12h4y1N7C8/?buvid=XXD548ADA1EC847DD55E8870EBE4E0EA7EC0A&from_spmid=main.my-history.0.0&is_story_h5=false&mid=79FBsywyK538EPOLx5FK2Q%3D%3D&p=1&plat_id=116&share_from=ugc&share_medium=android&share_plat=android&share_session_id=b62b79f3-c9f5-4697-a77e-21b09ba341f5&share_source=WEIXIN&share_tag=s_i&spmid=united.player-video-detail.0.0&timestamp=1712759182&unique_k=bDUK3wd&up_id=265006335的总结部分

然后，我们生成一个词，获取它的[1,hidden_units]的QKV向量，与KVcache结合，这就是我们所说的自回归生成阶段

考虑我们的mask是一个01矩阵

那么，对于一开始的prompt处理阶段，确实是需要一个mask的，就是普通的self-attention的mask

那么，对于新加入的词，它确实拥有全局的视野，它同时也看不到后面的未生成的词，因此它拥有这一轮的全局视野，如果这一轮(Prompt+生成词)的总个数为N，那么它就是一个1*N的全1向量

到下一轮，(Prompt+生成词)的总个数就为N+1了，此时它是一个[N*1,0]的向量，它什么都不用做就屏蔽了新的词，同时与它能看到的词计算了attention。
