## 1.Transformer简介

### 1.1 RNN

* 注意力机制允许神经网络学习序列中非常长的范围中的依赖关系
  * 这个范围远远长于LSTM（RNN的一种形式）
  * Attention机制是为RNN创造的，但是transformer只使用Attention，无需使用任何RNN单元来处理输入
* Transformer十分的耗费时间，但是不同于RNN，它可以并行计算



![image-20240229140632007](./assets/image-20240229140632007.png)

RNN是一种用于处理序列数据的神经网络，它通过在序列的每个时间步（t）上执行相同的任务，同时保持前一个输出的记忆。这种记忆称为隐藏状态（h），它允许网络捕获到目前为止的序列信息。

在这张图中，每个“RNN unit”代表网络在时间步t的一个单元，它接收两个输入：

1. **x(t)**：当前时间步的输入数据。
2. **h(t-1)**：前一个时间步的隐藏状态。

每个RNN单元根据这两个输入计算出两个输出：

1. **h(t)**：当前时间步的隐藏状态，它将被传递到下一个时间步作为输入。
2. 在图中未显示，但通常存在的输出，即当前时间步的输出（例如，用于分类的概率分布），这取决于特定应用。

RNN的关键特性是能够在隐藏状态中存储以前时间步的信息，并使用这种“记忆”影响网络的当前和未来的决策。这使得RNN特别适合于语言建模、时间序列分析、自然语言处理等需要处理序列数据的任务。



### 1.2 RNN的目标问题

RNN有多种形式，LSTM，GRU等

RNN的形式是随着其处理的目标问题相关联的，例如，

* 我们处理multi-input-single-output任务，例如垃圾邮件检测，RNN的形式是many-to-one

<img src="./assets/image-20240229141105917.png" alt="image-20240229141105917" style="zoom:50%;" />

* many-to-many task，例如文本生成

<img src="./assets/image-20240229141207113.png" alt="image-20240229141207113" style="zoom: 50%;" />



### 1.3. Seq2Seq:Encoder-Decoder

* 还有一个场景是翻译问题，其包含两个RNN需要解决的点

  * 输入输出长度不一致
  * RNN的$\hat{h}(t) = F(x_t,h(t-1))$​，这意味着实际上，这追求的是一个线性的关系，t时刻的信息只能由之前的得出，是一个标准的时间序列特性，然而翻译本身不遵守这个特性，某一个词的意思往往是基于上下文的。
  * 解决方法就是seq2seq

* seq2seq

  * 引入了编码器与解码器的概念

  ![image-20240229143602960](./assets/image-20240229143602960.png)

  * 编码器部分的任务是读取并处理输入序列（例如，一个句子）。在这个过程中，每个时间步对应于序列中的一个元素（通常是单词）。在处理序列时，编码器通过其RNN单元逐步构建一个内部状态。对于每个输入元素，都有一个嵌入表示（embedding），它将单词转换为向量形式，这样可以更有效地处理。

    在编码器处理完整个输入序列后，最终的内部状态（通常称为“上下文向量”或“思维向量”）被认为是输入序列的压缩表示。在图中，这表示为最后一个编码器RNN单元的输出h(T)。

    

  * 解码器部分接收编码器的最终状态并开始生成输出序列。在解码过程的开始，解码器通常接收一个特殊的开始符号（如"<GO>"），它表示输出序列的开始。在每个时间步，解码器RNN单元基于当前的内部状态和前一个时间步的输出（在训练时是真实的前一个单词，在推理时是模型自己生成的前一个单词）来生成下一个元素。

    解码器继续这个过程直到产生一个特殊的结束符号（如"<EOS>"），表示输出序列的结束。在每个时间步，解码器的输出可以是一个单词或者一个在更复杂任务中的其他类型的输出。

  * 在简单的编码器-解码器模型中，编码器产生的上下文向量是固定长度的。这意味着不管输入序列有多长，编码器总是将其压缩成一个固定大小的向量。这种方法的一个缺点是对于很长的序列，固定长度的上下文向量可能无法有效地捕获所有必要的信息。

    为了解决这个问题，注意力机制被引入到编码器-解码器架构中。注意力机制允许解码器在生成每个输出时动态地“关注”输入序列中不同的部分。这意味着解码器在每个时间步都可能基于不同的输入信息生成输出，而不是仅仅依赖于一个固定的上下文向量。这样，即使是长序列，模型也可以更好地处理，并且能够捕获更丰富的信息。

  * 一个简单的方式理解注意力

    * **不使用注意力机制的情况**：可以想象成在听某人讲述一长段故事，然后在故事讲完后立即尝试复述。在复述的每一步，我们都依赖于记忆中的故事内容以及我们已经复述的部分。这种方式可能对长段的信息回忆和复述具有挑战，尤其是当细节很多时。
    * **使用注意力机制的情况**：相比之下，使用注意力机制就像我们拥有了故事的完整副本，每次复述时，我们都能回头检视原故事的任何部分。这使得我们在复述每一个片段时都能集中注意力于故事的具体细节，即使这些细节分散在不同的地方。在翻译的过程中，这意味着模型可以“重新阅读”输入的句子的不同部分，并集中注意力于与当前生成的翻译最相关的部分。

    通过注意力机制，模型不必一次性记住所有输入信息，而是可以根据当前的上下文来动态决定输入的哪一部分最值得关注。这样做不仅提高了处理长序列的能力，还使得模型的决策过程更加透明可解释。

  * Decoder-Encoder的几种形式

    * 最后的隐藏状态（没有注意力机制）

      在不使用注意力机制的基本序列到序列模型中，编码器处理完输入序列后的最后一个隐藏状态（有时称为上下文向量或“思维向量”）被传递给解码器。解码器使用这个上下文向量作为其初始隐藏状态，并开始生成输出序列。在每个时间步，解码器都会基于当前的隐藏状态和前一个时间步生成的输出（或者在训练时是真实的前一个目标序列元素），来预测下一个元素。

    * 整个隐藏状态序列（带有注意力机制）

      当使用注意力机制时，解码器不仅接收最后的隐藏状态，还可以访问编码器的整个隐藏状态序列。在生成每个输出元素时，解码器通过注意力机制动态地加权这些隐藏状态，以便集中于与当前预测最相关的输入序列的不同部分。注意力权重决定了从编码器传递给解码器的信息的重点。



### 1.4 Attention in Seq2Seq(双向RNN)

对于编码器，我们通常使用Bi-directional RNN，这意味着我们不仅从正序的过一遍RNN，我们还倒序的过一遍RNN，这有助于我们对当前位置单词上下文关系的理解，而非像传统RNN一样我们只能得到“上文”信息。

![image-20240229151335732](./assets/image-20240229151335732.png)

对于解码器，就是根据之前的Input一次生成一个单词的输出

![image-20240229151712434](./assets/image-20240229151712434.png)

不同点在于，之前我们只会以Encoder的终态h(T)作为输入，现在我们添加了Context vector作为Output's input的一环。

这些以后再深入探究。

![image-20240229155439538](./assets/image-20240229155439538.png)

![image-20240229155452311](./assets/image-20240229155452311.png)









### 1.5 Transformer:Attention is all u need

这篇文章告诉你，forget abount RNN，只需要Attention就可以了。

通过这样做，我们仍然得到了翻译模型给最重要的特征，也就是每个输出都知道要注意哪个输入，因此，我们可以执行像机器翻译这样复杂的任务。

同时，RNN有两个主要的缺点：

* RNN很慢，更糟糕的是，你的下一个词一定是基于上一个词，因此不能并行化，
* 梯度消失，LSTM，GRU虽然做得很好，但是 实际上仍然存在一个极限长度，LSTM与GRU并不能很好的完成。

但是Transformer无论序列多长，其每个输入输出之间都会有直接的关系，我们不需要关心梯度消失。



唯一的缺点就是，假如Transformer只有一个Attention层，且Transformer使用attention但抛弃了RNN，那么attention的表本质上是n*m的attention weight。

更糟的是，Transformer拥有一堆Attention层，他们重复执行这么复杂的权重调整。











## 2. 情感分析(调用pipeline)

使用Hugging Face pipe line直接构建情感分析（已有的模型做二分类）。

情感分析本质是一个语言分类问题。

![image-20240229161328073](./assets/image-20240229161328073.png)

什么是pipline？

pipline旨在完成所有工作，包括tokenize text, converting text into integers（创建词表），使用这些词对应的整数进行推理。

```bash
pip install transformers
```

在HuggingFaceDocumentation有可用pipeline列表

```python
from transformers import pipeline
# return a model that can do prediction
classifier = pipeline("sentiment-analysis")

# 我们可以简单的输入raw text而非其他的张量来完成，因为pipeline已经为你完成了一切

classifier("This is such a great movie"，"No shit")
# return 一个字典，包括结果与置信度
```

